{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "askdocs",
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyPllWBb1+5sgwnw9vAxErv2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/brianchirn/askdocs/blob/main/askdocs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YqXpuxgvwJ-2"
      },
      "source": [
        "This code will pull the top 25 posts from the subreddit /r/askdocs and then try to use Natural Language Processing to summarize the post. \n",
        "\n",
        "Request use and obtain keys via https://www.reddit.com/prefs/apps "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1zwE-OBAuxD9"
      },
      "source": [
        "CLINET_ID = 'ysMKupnzIJk_hw' #personal use script\n",
        "SECRET_KEY = 'Dfe-vumHwUhs_rxUBulZYmZ3R4vb4Q'\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rdliRZAzxMvk"
      },
      "source": [
        "Request a temporary off token from reddit"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CnWTVjboxDB_"
      },
      "source": [
        "import requests\n",
        "auth = requests.auth.HTTPBasicAuth(CLINET_ID, SECRET_KEY)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "javvoEotyKGo"
      },
      "source": [
        "Login in to reddit with a dictionary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C7upoHOkyJu0"
      },
      "source": [
        "data = {\n",
        "    'grant_type' : 'password',\n",
        "    'username' : '_123test',\n",
        "    'password' : 'password'\n",
        "}\n",
        "\n",
        "headers = {'User-Agent': 'MyAPI/0.0.1'}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YxbQyxFR_9oW"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UDMQCSjUywQt"
      },
      "source": [
        "Send a request for our auth token.\n",
        "We are requesting the version 1 of the api, the access_token\n",
        "\n",
        "We need to include our auth, data and headers\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bfllGyodxJOn"
      },
      "source": [
        "res = requests.post('https://www.reddit.com/api/v1/access_token',\n",
        "                    auth = auth, data = data, headers = headers)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SVK-2dr_x79Q"
      },
      "source": [
        "res.json()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zXvaboi20VDA"
      },
      "source": [
        "TOKEN = res.json()['access_token']\n",
        "#reformat headers\n",
        "headers = {**headers, **{'Authorization': f'bearer {TOKEN}'}}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1o-1L-1x0x53"
      },
      "source": [
        "headers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6DRbNQWA1I4p"
      },
      "source": [
        "We are now able to access API on reddit as long as we have headers = headers\n",
        "\n",
        "Now let's access all of the hot posts within a subreddit\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "83C8nUa91Mde"
      },
      "source": [
        "res = requests.get('https://oauth.reddit.com/r/askdocs/top', \n",
        "                 headers = headers, params = {'limit': 25})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nvijjMJI19cM"
      },
      "source": [
        "for post in res.json()['data']['children']:\n",
        "  print(post['data'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "md0SNmNx2lVR"
      },
      "source": [
        "Let's convert this into pandas dataframe\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LGS_LVkv2rAM"
      },
      "source": [
        "import pandas as pd\n",
        "df = pd.DataFrame()\n",
        "\n",
        "for post in res.json()['data']['children']:\n",
        "  df = df.append({\n",
        "      'subreddit' : post['data']['subreddit'],\n",
        "      'title' : post['data']['title'],\n",
        "      'selftext' : post['data']['selftext'],\n",
        "      'upvote_ratio': post['data']['upvote_ratio'],\n",
        "      'ups': post['data']['ups'],\n",
        "      'length': len(post['data']['selftext'])\n",
        "\n",
        "  }, ignore_index = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5CKACTvD3qUB"
      },
      "source": [
        "post['data'].keys() #see what info you want to extract from post\n",
        "df\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CzIUTLmt5WUe"
      },
      "source": [
        "import re\n",
        "test = df['title'][14]+ df['selftext'][14]\n",
        "test = (re.sub('[()]', '', test)) #remove parathesis\n",
        "test = re.sub(r\"[\\[\\]]\", \"\", test) #remove brackets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a-6766w6hhIE"
      },
      "source": [
        "Using NLTK to summarize text \n",
        "code taking from here:\n",
        "https://stackabuse.com/text-summarization-with-nltk-in-python/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s0DNDPw4iD5R"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "sentence_list = nltk.sent_tokenize(test)\n",
        "sentence_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JF90cyEtipS9"
      },
      "source": [
        "nltk.download('stopwords')\n",
        "stopwords = nltk.corpus.stopwords.words('english')\n",
        "word_frequencies = {}\n",
        "\n",
        "#modify code to weigh numbers higher than normal words\n",
        "def hasNumbers(inputString):\n",
        "  return any(char.isdigit() for char in inputString)\n",
        "\n",
        "for word in nltk.word_tokenize(test):\n",
        "  if word not in stopwords:\n",
        "    if word not in word_frequencies.keys():\n",
        "      if hasNumbers(word) == True:\n",
        "        word_frequencies[word] = 5\n",
        "      else:\n",
        "        word_frequencies[word] = 1\n",
        "    else:\n",
        "      word_frequencies[word] += 1\n",
        "print(word_frequencies)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_6eZYsOWjbJs"
      },
      "source": [
        "maximum_frequency = max(word_frequencies.values())\n",
        "for word in word_frequencies.keys():\n",
        "  word_frequencies[word] = (word_frequencies[word]/maximum_frequency)\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A87HSEWUjyA9"
      },
      "source": [
        "sentence_scores = {}\n",
        "for sent in sentence_list:\n",
        "  for word in nltk.word_tokenize(sent.lower()):\n",
        "    if word in word_frequencies.keys():\n",
        "      if len(sent.split(' ')) < 30:\n",
        "        if sent not in sentence_scores.keys():\n",
        "          sentence_scores[sent] = word_frequencies[word]\n",
        "        else:\n",
        "          sentence_scores[sent]+= word_frequencies[word]\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-TAhs_V_kbvf"
      },
      "source": [
        "import heapq\n",
        "summary_sentences = heapq.nlargest(7, sentence_scores, key = sentence_scores.get)\n",
        "print('SUMMARY')\n",
        "print(len(summary))\n",
        "summary = '\\n'.join(summary_sentences)\n",
        "print(summary)\n",
        "\n",
        "\n",
        "\n",
        "print('\\nORIGINAL')\n",
        "print(len(test))\n",
        "test"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}